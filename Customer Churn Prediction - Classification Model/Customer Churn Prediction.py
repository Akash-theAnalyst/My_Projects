# -*- coding: utf-8 -*-
"""Code_Akash_Jayakumar.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vOv9Q-NL-_DNq-RtMFm1LcEAKxgIZuBI
"""

# CA1 - Predict customer churn using given Dataset.

from pandas import read_csv, get_dummies, Series, DataFrame
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE

dataset1 = read_csv("/content/CustomerChurn.csv") # Called file from device rather than drive because i unlinked my drive access.

# print(dataset1)
# print(dataset1.head())
# print(dataset1.tail())
# print(dataset1.describe())
# print(dataset1.info())
# print(dataset1.shape)

dataset1['Attrition_Flag'] = dataset1['Attrition_Flag'].map({"Existing Customer" :1, "Attrited Customer" : 0})
dataset1['Gender'] = dataset1['Gender'].map({"M" :1, "F" : 0})

dataset2 = get_dummies(dataset1,['Education_Level', 'Marital_Status', 'Income_Category', 'Card_Category'])
dataset2.info()

x = dataset2.drop('Attrition_Flag',axis=1)
y = dataset2['Attrition_Flag']

# print(x)
# print(y)

x_scaled = StandardScaler().fit_transform(x)

x_train,x_test,y_train,y_test=train_test_split(x_scaled,y,test_size=0.3, random_state=100)

x_train,y_train = SMOTE(random_state=20).fit_resample(x_train,y_train)

# print(x_train)
# print(y_train)

# Using Pipeline and GridSearch Method

from imblearn.pipeline import Pipeline
from sklearn import ensemble
from sklearn.model_selection import GridSearchCV

#x1 = dataset2[['Total_Trans_Ct','Total_Trans_Amt','Total_Revolving_Bal']]
#x1_scaled = StandardScaler().fit_transform(x1)

RF_classifier1 = Pipeline([('balancing', SMOTE(random_state = 150)),('classification', ensemble.RandomForestClassifier(criterion='entropy', max_features='auto', random_state=1))])
no_trees = {'classification__n_estimators': [150,200,250,300,350,400]}

grid_search = GridSearchCV(estimator=RF_classifier1, param_grid=no_trees, scoring='precision', cv=5)
grid_search.fit(x_scaled, y)

best_parameters = grid_search.best_params_
best_result = grid_search.best_score_

print("Best Parameter: ",best_parameters)
print("Precision: ", best_result)

# Best Parameter:  {'classification__n_estimators': 350}
# Precision:  0.9393522901157734

# Random Forest Classifier - Method 1

RF_classifier2 = ensemble.RandomForestClassifier(n_estimators=350, criterion='entropy', max_features='auto', random_state=1)

RF_classifier2.fit(x_train,y_train)
y_pred2 = RF_classifier2.predict(x_test)

imp_features = Series(RF_classifier2.feature_importances_, index=list(x)).sort_values(ascending=False)
print(imp_features)

# Total_Trans_Ct                    0.228018
# Total_Trans_Amt                   0.177853
# Total_Revolving_Bal               0.131902

# Accuracy and Confusion matrix

from sklearn import metrics

Accuracy = metrics.accuracy_score(y_test, y_pred2)
precision = metrics.recall_score(y_test, y_pred2)

print("Accuracy: ", Accuracy)
print ("Precision: ", precision)

# Accuracy:  0.936965811965812
# Precision:  0.9627488760436738

# Grid Search with Best Features

x2 = dataset2[['Total_Trans_Ct','Total_Trans_Amt','Total_Revolving_Bal']]
x2_scaled = StandardScaler().fit_transform(x2)

RF_classifier3 = ensemble.RandomForestClassifier(criterion='entropy', max_features='auto', random_state=1)
no_trees = {'n_estimators': [200, 250, 300, 350, 400, 450]}

grid_search2 = GridSearchCV(estimator=RF_classifier3, param_grid=no_trees, scoring='precision', cv=5)
grid_search2.fit(x2_scaled, y)

best_parameters = grid_search2.best_params_
best_result = grid_search2.best_score_

print("Best Parameters: ", best_parameters)
print("Precision: ", best_result)

# Best Parameters:  {'n_estimators': 300}
# Precision:  0.9196999750921586

# Random Forest Classifier - Method 1 Again

RF_classifier4 = ensemble.RandomForestClassifier(n_estimators=300, criterion='entropy', max_features='auto', random_state=1)

RF_classifier4.fit(x_train,y_train)
y_pred3 = RF_classifier4.predict(x_test)

imp_features = Series(RF_classifier2.feature_importances_, index=list(x)).sort_values(ascending=False)
print(imp_features)

# Total_Trans_Ct                    0.228018
# Total_Trans_Amt                   0.177853
# Total_Revolving_Bal               0.131902

# Accuracy and Confusion matrix Again

from sklearn import metrics

Accuracy = metrics.accuracy_score(y_test, y_pred3)
print("Accuracy: ", Accuracy)

precision = metrics.recall_score(y_test, y_pred3)
print ("Precision: ", precision)

# Accuracy:  0.936965811965812
# Precision:  0.9627488760436738

# Grid Search with Best Features Again

x3 = dataset2[['Total_Trans_Ct','Total_Trans_Amt','Total_Revolving_Bal']]
x3_scaled = StandardScaler().fit_transform(x3)

RF_classifier5 = ensemble.RandomForestClassifier(criterion='entropy', max_features='auto', random_state=1)
no_trees = {'n_estimators': [250,260,270,280,290,300,310,320,330,340,350,360,370,380,390,400]}

grid_search2 = GridSearchCV(estimator=RF_classifier5, param_grid=no_trees, scoring='precision', cv=5)
grid_search2.fit(x3_scaled, y)

best_parameters = grid_search2.best_params_
best_result = grid_search2.best_score_

print("Best Parameters: ", best_parameters)
print("Precision: ", best_result)

# Best Parameters:  {'n_estimators': 270}
# Precision:  0.9201053288636242

# Random Forest Classifier - Method 1 Again with 270 as value

RF_classifier6 = ensemble.RandomForestClassifier(n_estimators=270, criterion='entropy', max_features='auto', random_state=1)

RF_classifier6.fit(x_train,y_train)
y_pred4 = RF_classifier6.predict(x_test)

imp_features = Series(RF_classifier2.feature_importances_, index=list(x)).sort_values(ascending=False)
print(imp_features)

# Total_Trans_Ct                    0.228018
# Total_Trans_Amt                   0.177853
# Total_Revolving_Bal               0.131902

# Accuracy and Confusion matrix Again for 270 as value

from sklearn import metrics

Accuracy = metrics.accuracy_score(y_test, y_pred4)
print("Accuracy: ", Accuracy)

precision = metrics.recall_score(y_test, y_pred4)
print ("Precision: ", precision)

# Accuracy:  0.9391025641025641
# Precision:  0.9633911368015414

# This line Shows a slight Difference a better Accuracy and Precision although the increase is minimal, There is a increase.

# Using Pipline and Gridsearch model to get a better result and model for 270

x4 = dataset2[['Total_Trans_Ct','Total_Trans_Amt','Total_Revolving_Bal']]
x_scaled4 = StandardScaler().fit_transform(x4)

RF_classifier7 = Pipeline([('balancing', SMOTE(random_state = 150)),('classification', ensemble.RandomForestClassifier(criterion='entropy', max_features='auto', random_state=1))])
no_trees = {'classification__n_estimators': [240,245,250,255,260,265,270,275,280,285,290,295,300]}

grid_search = GridSearchCV(estimator=RF_classifier7, param_grid=no_trees, scoring='precision', cv=5)
grid_search.fit(x_scaled4, y)

best_parameters = grid_search.best_params_
best_result = grid_search.best_score_

print("Best Parameters: ", best_parameters)
print("Precision: ", best_result)

# Best Parameters:  {'classification__n_estimators': 285}
# Precision:  0.9344217896764787

# Random Forest Classifier - Method 1 Again with 285 as value

RF_classifier7 = ensemble.RandomForestClassifier(n_estimators=285, criterion='entropy', max_features='auto', random_state=1)

RF_classifier7.fit(x_train,y_train)
y_pred5 = RF_classifier7.predict(x_test)

imp_features = Series(RF_classifier7.feature_importances_, index=list(x)).sort_values(ascending=False)
print(imp_features)

# Total_Trans_Ct                    0.229187
# Total_Trans_Amt                   0.176418
# Total_Revolving_Bal               0.132253

# Accuracy and Confusion matrix Again for 285 as value

from sklearn import metrics

Accuracy = metrics.accuracy_score(y_test, y_pred5)
print("Accuracy: ", Accuracy)

precision = metrics.recall_score(y_test, y_pred5)
print ("Precision: ", precision)

# Accuracy:  0.9385683760683761
# Precision:  0.9633911368015414

# Using Pipline and Gridsearch model to get a better result and model for a wide range of 250 to 310 values

x5 = dataset2[['Total_Trans_Ct','Total_Trans_Amt','Total_Revolving_Bal']]
x_scaled5 = StandardScaler().fit_transform(x5)

RF_classifier8 = Pipeline([('balancing', SMOTE(random_state = 150)),('classification', ensemble.RandomForestClassifier(criterion='entropy', max_features='auto', random_state=1))])
no_trees = {'classification__n_estimators': [250,255,260,265,270,275,280,285,290,295,300,305,310]}

grid_search = GridSearchCV(estimator=RF_classifier8, param_grid=no_trees, scoring='precision', cv=5)
grid_search.fit(x_scaled5,y)

best_parameters = grid_search.best_params_
best_result = grid_search.best_score_

print("Best Parameters: ", best_parameters)
print("Precision: ", best_result)

# Best Parameters:  {'classification__n_estimators': 285}
# Precision:  0.9344217896764787

# still getting same set of results and stuck with same precision.

# Support Vector Classifier -  Method 3

from sklearn import svm

SVM_classifier1 = Pipeline([('balancing', SMOTE(random_state = 100)),('classification', svm.SVC(random_state=10))])
kernels_c = {'classification__kernel': ['linear','poly','rbf','sigmoid'], 'classification__C': [.001,.01,.1,1,10,100]}

grid_search2 = GridSearchCV(estimator=SVM_classifier1, param_grid=kernels_c, scoring='precision', cv=5)
grid_search2.fit(x_scaled, y)

best_parameters = grid_search2.best_params_
best_result = grid_search2.best_score_

print("Best Parameters: ", best_parameters)
print("Precision: ", best_result)

# Best Parameters:  {'classification__C': 0.001, 'classification__kernel': 'sigmoid'}
# Precision:  0.9723519613005995

"""## **I Intend to Reduce False Positive and that is the reason my Model is focused on Precision, Because Recall is Tolerable, But on the other hand Precision is not and Here the Support Vector Method has better Precision of about 93% using pipeline and around 97% when using random forest method 1 compared to 93% Precision we got from Random Forest Method.**"""